import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
plt.ion()
from skimage.transform import resize
import mrcfile
import time
from torchsummary import summary
from ops.radon_3d_lib import ParallelBeamGeometry3DOpAngles, ParallelBeamGeometry3DOpAngles_rectangular
import os
import imageio
import torch.nn.functional as F
import torch.nn as nn

from utils import data_generation, reconstruction, utils_deformation, utils_sampling, utils_interpolation, utils_display, utils_ricardo
from utils.reconstruction import getfsc

from torch.utils.data import DataLoader, TensorDataset
from utils.utils_sampling import sample_implicit, generate_rays_batch, sample_implicit_batch_lowComp, generate_ray, generate_rays_batch_bilinear

from models.fourier_net import FourierNet,FourierNet_Features, MLP
import tinycudann as tcnn
import json

import warnings
warnings.filterwarnings('ignore') 

# Introduction
'''
This script is used to generate data for AreTomo. The final goal is to use the same data generated by this script with our
method to compare with AreTomo.
'''

torch_type=torch.float
use_cuda=torch.cuda.is_available()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu") 
if torch.cuda.device_count()>1:
    torch.cuda.set_device(0)
seed = 42
np.random.seed(seed)
torch.manual_seed(seed)



## TODO: load data from config file
volume_name = 'model_0'

# Parameters for the data generation
# size of the volume to use to generate the tilt-series
n1 = 512
n2 = 512
n3 = 180 # size of the effective volume
# size of the patch to crop in the raw volume
n1_patch = 512
n2_patch = 512
n3_patch = 180 # size of the effective volume
# nZ = 512 # size of the extended volume
Nangles = 61
view_angle_min = -60
view_angle_max = 60
SNR_value = 10
sigma_PSF = 3.
number_sub_projections = 1

scale_min = 1.0
scale_max = 1.0
shift_min = -0.04
shift_max = 0.04
shear_min = -0.0
shear_max = 0.0
angle_min = -4/180*np.pi
angle_max = 4/180*np.pi
sigma_local_def = 4
N_ctrl_pts_local_def = (12,12)


# # Parameters for the data generation
path_load = "./results/"+str(volume_name)+"_SNR_"+str(SNR_value)+"_size_"+str(n1)+"_no_PSF/"
path_save = "./results/model0_SNR_"+str(10)+"_size_"+str(512)+"_no_PSF/"

## TODO: use folloying to check that the load data are consistent with the current parameters
# params = np.load(path_load+"parameters.npz")
# n1 = params['n1'].item()
# n2 = params['n2'].item()
# n3 = params['n3'].item()
# SNR_value = params['SNR_value'].item()
# sigma_PSF = params['sigma_PSF'].item()
# angle_bound = params['angle_bound'].item()
# Nangles = params['Nangles'].item()

# grid_class = utils_sampling.grid_class(n1,n2,n3,torch_type,device)
# us = 4 # under sample factor
# grid_class_us = utils_sampling.grid_class(n1//us,n2//us,n3//us,torch_type,device)


# path_save = "./results/model0_SNR_"+str(SNR_value)+"_size_"+str(n1)+"/"
if not os.path.exists("results/"):
    os.makedirs("results/")
if not os.path.exists(path_save):
    os.makedirs(path_save)
if not os.path.exists(path_save+"projections/"):
    os.makedirs(path_save+"projections/")
if not os.path.exists(path_save+"grid/"):
    os.makedirs(path_save+"grid/")
if not os.path.exists(path_save+"deformations/"):
    os.makedirs(path_save+"deformations/")
if not os.path.exists(path_save+"deformations/training/"):
    os.makedirs(path_save+"deformations/training/")
if not os.path.exists(path_save+"projections/training/"):
    os.makedirs(path_save+"projections/training/")
if not os.path.exists(path_save+"projections/volume/"):
    os.makedirs(path_save+"projections/volume/")
if not os.path.exists(path_save+"training/"):
    os.makedirs(path_save+"training/")

## TODO: do we need all the following for training?
data = np.load(path_load+"volume_and_projections.npz")
projections_noisy = torch.tensor(data['projections_noisy']).type(torch_type).to(device)
projections_deformed = torch.tensor(data['projections_deformed']).type(torch_type).to(device)
projections_deformed_global = torch.tensor(data['projections_deformed_global']).type(torch_type).to(device)
projections_clean = torch.tensor(data['projections_clean']).type(torch_type).to(device)
PSF = torch.tensor(data['PSF']).type(torch_type).to(device)
if sigma_PSF!=0:
    supp_PSF = max(PSF.shape)

affine_tr = np.load(path_load+"global_deformations.npy",allow_pickle=True)
local_tr = np.load(path_load+"local_deformations.npy", allow_pickle=True)

V_t = torch.tensor(np.moveaxis(np.double(mrcfile.open(path_load+"V.mrc").data),0,2)).type(torch_type).to(device)
V_FBP_no_deformed_t = torch.tensor(np.moveaxis(np.double(mrcfile.open(path_load+"V_FBP_no_deformed.mrc").data),0,2)).type(torch_type).to(device)
V_FBP =  torch.tensor(np.moveaxis(np.double(mrcfile.open(path_load+"V_FBP.mrc").data),0,2)).type(torch_type).to(device)

V_ = V_t.detach().cpu().numpy()
V_FBP_ = V_FBP.detach().cpu().numpy()
V_FBP_no_deformed = V_FBP_no_deformed_t.detach().cpu().numpy()

V_ /= V_.sum()
V_FBP_ /= V_FBP_.sum()
V_FBP_no_deformed /= V_FBP_no_deformed.sum()
fsc_FBP = utils_ricardo.FSC(V_,V_FBP_)
fsc_FBP_no_deformed = utils_ricardo.FSC(V_,V_FBP_no_deformed)
x_fsc = np.arange(fsc_FBP.shape[0])


######################################################################################################
######################################################################################################
##
## TRAINING
##
######################################################################################################
######################################################################################################

# Estimate Volume from the deformed projections
train_all = True # train or load model
learn_volume = True # learn the volume
learn_global = True # learn global dn3/max(n1,n2)/np.cos((90-angle_bound)*np.pi/180)eformation
learn_local = True # learn local deformation
local_model = 'interp' #  'implicit' or 'interp'
initialize_local_def = False
initialize_volume = False
use_deformation_estimation = True # estimate deformation, useful to see what happens when we don't
 
model_type = 2 #0 for Fourier feature, 1 for MLP

batch_size = 10 # number of viewing direction per iteration
nRays =  1500 # number of sampling rays per viewing direction
# ray_length = 512 # number of points along one ray
z_max = 2*n3/max(n1,n2)/np.cos((90-angle_bound)*np.pi/180)
ray_length = 500#int(np.floor(n1*z_max))

## Parameters
epochs = 400
Ntest = 25 # number of epoch before display
# iter_local = 1000000 # include training of local deformations after few epochs
# frac = 1
lr_volume = 1e-2
lr_deformations =1e-4
lr_local_def = 1e-4
lr_shift = 1e-3
lr_rot = 1e-3
lamb_volume = 0*1e-5 # regul parameters on volume regularization
lamb_volume_out = 0*1e-0 # regul parameters on volume regularization to be 0 outside domain
lamb_local = 0*1e-3 # regul parameters on local deformation
lamb_local_smooth = 0*1e-8 # regul parameters on local deformation to be smooth
lamb_local_ampl = 1e2 # regul on amplitude of local def.
lamb_rot = 1e-6 # regul parameters on inplane rotations
lamb_shifts = 1e-6 # regul parameters on shifts
wd = 5e-6 # weights decay
Nangles_ = Nangles
scheduler_step_size = 200
scheduler_gamma = 0.6
delay_deformations = 25 # Delay before learning deformations


# Params of implicit deformation
deformationScale = 1
inputRange = 1
Npts_rd = 500 # number of random points to compute regul

# if implicit model
input_size = 2
output_size = 2
num_layers = 3
hidden_size = 32
L = 10
# if interpolation model
N_ctrl_pts_net = 20

# params of implicit volume
input_size_volume = 3
output_size_volume = 1
num_layers_volume = 4
hidden_size_volume = 128
L_volume = 3

mollifier = utils_sampling.mollifier_class(-1,torch_type,device)
barycenter_true = torch.zeros(3).type(torch_type).to(device)

######################################################################################################
## Define the volume
# from models.fourier_net import FourierNet
# impl_volume = FourierNet(
#     in_features=input_size_volume,
#     out_features=output_size_volume,
#     hidden_features=hidden_size_volume,
#     hidden_blocks=num_layers_volume,
#     L = L_volume).to(device)  
# num_param = sum(p.numel() for p in impl_volume.parameters() if p.requires_grad) 
# print('---> Number of trainable parameters in volume net: {}'.format(num_param))


# from models.fourier_net import MultiResImplicitRepresentation
# nFeature_volume = 32
# res_volume = (8,16,32,64)
# impl_volume = MultiResImplicitRepresentation(d=3, nFeature=nFeature_volume, res=res_volume, L=L,
#                                       hidden_features=hidden_size_volume,
#                                        hidden_blocks=num_layers_volume,
#                                         out_features=1).cuda()

# num_param = sum(p.numel() for p in impl_volume.parameters() if p.requires_grad) 
# print('---> Number of trainable parameters in volume net: {}'.format(num_param))

config = {"encoding": {
        'otype': 'Grid',
        'type': 'Hash',
        'n_levels': 8,
        'n_features_per_level': 2,
        'log2_hashmap_size': 24,
        'base_resolution': 4,
        'per_level_scale': 2,
        'interpolation': 'Smoothstep'
	}}

try: 
    grid = tcnn.Encoding(3,encoding_config=config['encoding'],dtype=torch.float32).to(device)
    sub_features = grid.n_output_dims
    # print(sub_features)
except:
    print("Error")

grid = tcnn.Encoding(3,encoding_config=config['encoding'],dtype=torch.float32).to(device)
sub_features = grid.n_output_dims
# print(sub_features)
if(model_type==0):
    impl_volume_net = FourierNet_Features(
        in_features=input_size_volume,
        sub_features=sub_features,
        out_features=output_size_volume, 
        hidden_features=hidden_size_volume,
        hidden_blocks=num_layers_volume,
        L = L_volume).to(device)

    class VolumeNet(torch.nn.Module):
        def __init__(self,grid, impl_volume):
            super(VolumeNet, self).__init__()
            self.grid = grid
            self.impl_volume = impl_volume
        def forward(self, xs):
            xx = self.grid(xs)
            #print(xx.shape)
            xs = torch.cat((xs, xx), dim=1)
            #print(xs.shape)
            return self.impl_volume(xs)
    impl_volume = VolumeNet(grid, impl_volume_net).to(device)
        
if(model_type==1):
    impl_volume_net = MLP(in_features= grid.n_output_dims, 
                          hidden_features=hidden_size_volume, hidden_blocks= num_layers_volume, out_features=output_size_volume).to(device)
    class VolumeNet(torch.nn.Module):
        def __init__(self,grid, impl_volume):
            super(VolumeNet, self).__init__()
            self.grid = grid
            self.impl_volume = impl_volume
        def forward(self, xs):
            gridOp = self.grid(xs)
            return self.impl_volume(gridOp) 

    impl_volume = VolumeNet(grid, impl_volume_net).to(device)

if(model_type==2):  
    config = {"encoding": {
            'otype': 'Grid',
            'type': 'Hash',
            'n_levels': 9,
            'n_features_per_level': 2,
            'log2_hashmap_size': 20,
            'base_resolution': 8,
            'per_level_scale': 2,
            'interpolation': 'Smoothstep'
        },
        "network": {
            "otype": "FullyFusedMLP",   
            "activation": "ReLU",       
            "output_activation": "None",
            "n_neurons": hidden_size_volume,           
            "n_hidden_layers": num_layers_volume,       
        }       
        }
    impl_volume = tcnn.NetworkWithInputEncoding(n_input_dims=3, n_output_dims=1, encoding_config=config["encoding"], network_config=config["network"]).to(device)




num_param = sum(p.numel() for p in impl_volume.parameters() if p.requires_grad) 
print('---> Number of trainable parameters in volume net: {}'.format(num_param))


######################################################################################################
## Define the implicit deformations
if local_model=='implicit':
    # Define Implicit representation of local deformations
    implicit_deformation_list = []
    for k in range(Nangles_):
        implicit_deformation = FourierNet(
            in_features=input_size,
            out_features=output_size,
            hidden_features=hidden_size,
            hidden_blocks=num_layers,
            L = L).to(device)
        implicit_deformation_list.append(implicit_deformation)

    num_param = sum(p.numel() for p in implicit_deformation_list[0].parameters() if p.requires_grad) 
    print('---> Number of trainable parameters in implicit net: {}'.format(num_param))
elif 'interp':
    depl_ctr_pts_net = torch.zeros((2,N_ctrl_pts_net,N_ctrl_pts_net)).to(device).type(torch_type)/max([n1,n2,n3])/10
    implicit_deformation_list = []
    for k in range(Nangles_):
        # depl_ctr_pts_net = local_tr[k].depl_ctr_pts.clone().detach()[0].cuda()/deformationScale
        field = utils_deformation.deformation_field(depl_ctr_pts_net.clone(),maskBoundary=2)
        implicit_deformation_list.append(field)
    num_param = sum(p.numel() for p in implicit_deformation_list[0].parameters() if p.requires_grad) 
    print('---> Number of trainable parameters in implicit net: {}'.format(num_param))


######################################################################################################
## Define the global deformations
shift_est = []
rot_est = []
for k in range(Nangles_):
    shift_est.append(utils_deformation.shiftNet(1).to(device))
    rot_est.append(utils_deformation.rotNet(1).to(device))




######################################################################################################
# Optimizer
loss_data = nn.L1Loss()

# list_params = []
# list_params_with_local = []
list_params_deformations = []
if(use_deformation_estimation):
    for k in range(Nangles_):
        if learn_global:
            list_params_deformations.append({"params": shift_est[k].parameters(), "lr": lr_shift})
            list_params_deformations.append({"params": rot_est[k].parameters(), "lr": lr_rot})
        if learn_local:
            list_params_deformations.append({"params": implicit_deformation_list[k].parameters(), "lr": lr_local_def})

learn_deformations = False
optimizer_volume = torch.optim.Adam(impl_volume.parameters(), lr=lr_volume, weight_decay=wd)
optimizer_deformations = torch.optim.Adam(list_params_deformations, lr=lr_deformations, weight_decay=wd)

scheduler_volume = torch.optim.lr_scheduler.StepLR(optimizer_volume, step_size=scheduler_step_size, gamma=scheduler_gamma)
if(use_deformation_estimation):
    scheduler_deformation = torch.optim.lr_scheduler.StepLR(
        optimizer_deformations, step_size=scheduler_step_size, gamma=scheduler_gamma)

######################################################################################################
# Format data for batch training
index = torch.arange(0, Nangles, dtype=torch.long) # index for the dataloader

# Define dataset
angles = np.linspace(-angle_bound,angle_bound,Nangles)
angles_t = torch.tensor(angles).type(torch_type).to(device)
dataset = TensorDataset(angles_t,projections_noisy.detach(),index)
trainLoader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)


######################################################################################################
## Track sampling
choosenLocations_all = {}
for a in angles:
    choosenLocations_all[a] = []

current_sampling = np.ones_like(projections_noisy.detach().cpu().numpy())

# x = np.arange(0,n1)
# y = np.arange(0,n2)
# X, Y = np.meshgrid(x, y)
# loc_grid = torch.Tensor(np.stack([X,Y],axis = 2).reshape(-1,2)).long().to(device)

# loc_grid_3D = loc_grid_3D = grid_class.grid3d_t
# volLoader = DataLoader(loc_grid_3D, batch_size=10000, shuffle=False)
            
    
if sigma_PSF!=0:        
    psf_shift = torch.zeros((supp_PSF,supp_PSF)).type(torch_type).to(device)
    xx_ = torch.tensor(np.arange(-supp_PSF//2,supp_PSF//2)/n1).type(torch_type).to(device)
    yy_ = torch.tensor(np.arange(-supp_PSF//2,supp_PSF//2)/n2).type(torch_type).to(device)
    psf_shift_x,psf_shift_y = torch.meshgrid(xx_,yy_)
    psf_shift_x = psf_shift_x.reshape(1,1,-1,1)
    psf_shift_y = psf_shift_y.reshape(1,1,-1,1)

    PSF_ext = PSF.reshape(1,1,-1,1)


######################################################################################################
## Iterative optimization
if train_all:
    loss_tot = []
    loss_data_fidelity = []
    loss_regul_local_smooth = []
    loss_regul_local_ampl = []
    loss_regul_volume = []
    loss_regul_shifts = []
    loss_regul_rot = []
    SNR_tot = []
    t_test = []
    t0 = time.time()
    for ep in range(epochs):
        if(ep>delay_deformations):
            learn_deformations = True
        # if ep%int(0.2*epochs)==0 and ep!=0:
        #     frac += 0.2
        #     print("###################")
        #     print("######  Now use {}% of the feature encoding".format(int(np.round(frac*100,0))))
        for   angle,proj, idx_loader  in trainLoader:
            optimizer_volume.zero_grad()
            if learn_deformations:
                optimizer_deformations.zero_grad()

            # Choosing the subset of the paramters
            if(use_deformation_estimation):
                rot_deformSet= list(map(rot_est.__getitem__, idx_loader))
                shift_deformSet= list(map(shift_est.__getitem__, idx_loader))
                local_deformSet= list(map(implicit_deformation_list.__getitem__, idx_loader))
            else:
                rot_deformSet = None
                shift_deformSet = None
                local_deformSet = None

            # Generating rays
            raysSet,raysRot, isOutsideSet, pixelValues = generate_rays_batch_bilinear(proj,angle,nRays,ray_length,
                                                                                                randomZ=2,zmax=z_max,
                                                                                                choosenLocations_all=choosenLocations_all)

            if sigma_PSF!=0:
                raysSet_ = raysSet.reshape(batch_size,nRays,1,ray_length,3).repeat(1,1,supp_PSF**2,1,1)
                raysSet_[:,:,:,:,0] = raysSet_[:,:,:,:,0]+psf_shift_x
                raysSet_[:,:,:,:,1] = raysSet_[:,:,:,:,1]+psf_shift_y
                raysSet = raysSet_.reshape(batch_size,nRays*supp_PSF**2,ray_length,3)

            outputValues,support = sample_implicit_batch_lowComp(impl_volume,raysSet,angle,
                rot_deformSet=rot_deformSet,shift_deformSet=shift_deformSet,local_deformSet=local_deformSet,
                scale=deformationScale,range=inputRange,zlimit=n3/max(n1,n2))
            
            outputValues = outputValues.type(torch_type)

            if sigma_PSF!=0:
                outputValues = (outputValues.reshape(batch_size,nRays,supp_PSF**2,ray_length)*PSF_ext).sum(2)

                support = support.reshape(outputValues.shape[0],outputValues.shape[1],supp_PSF**2,-1)
                support = support[:,:,supp_PSF**2//2,:] # take only the central elements
            else:
                support = support.reshape(outputValues.shape[0],outputValues.shape[1],-1)
                
            projEstimate = torch.sum(support*outputValues,2)/n3
            loss = loss_data(projEstimate,pixelValues.to(projEstimate.dtype))

            loss_data_fidelity.append(loss.item())


            # update sampling
            with torch.no_grad():
                for jj, ii_ in enumerate(idx_loader):
                    ii = ii_.item()
                    idx = np.floor((choosenLocations_all[angles[ii]][-1]+1)/2*max(n1,n2)).astype(np.int)
                    current_sampling[ii,idx[:,0],idx[:,1]] += 1

            ## Add regularizations
            if(use_deformation_estimation):
                # local deformations
                if learn_local:
                    for ii in idx_loader.detach().cpu().numpy():
                        if lamb_local_smooth!=0 or lamb_local_ampl!=0:
                            if Npts_rd<grid_class.grid2d_t.shape[0]:
                                tmp = np.random.choice(np.arange(grid_class.grid_ball2d_t.shape[0]),size=Npts_rd,replace=False)
                                grid_rd = grid_class.grid_ball2d_t[tmp]
                            else:
                                grid_rd = grid_class.grid2d_t.clone()

                            depl = torch.abs(implicit_deformation_list[ii](grid_rd))

                            loss += lamb_local_ampl*(depl.mean())
                            loss_regul_local_ampl.append(lamb_local_ampl*depl.mean().item())

                        # if lamb_local_smooth!=0:
                        #     # define convolution filter   
                        #     supp_ = 2
                        #     G = np.zeros((n1,n2))
                        #     # G[n1//2-supp_//2:n1//2+supp_//2,n2//2] = 1
                        #     G[n1//2,n2//2-1] = 1
                        #     G[n1//2,n2//2] = 1
                        #     G /= G.sum()
                        #     G = torch.tensor(G[:,:,None]).to(device).type(torch_type)
                        #     tmp_corr1 = torch.fft.ifftshift(torch.fft.ifft2(torch.fft.fft2(G,dim=(0,1))*torch.fft.fft2(depl,dim=(0,1)),dim=(0,1)),dim=(0,1)).real
                        #     tmp_corr1 = torch.abs(tmp_corr1.sum(2))/((depl**2).sum(2)+1e-6)
                        #     G = np.zeros((n1,n2))
                        #     # G[n1//2,n2//2-supp_//2:n2//2+supp_//2] = 1
                        #     G[n1//2-1,n2//2] = 1
                        #     G[n1//2,n2//2] = 1
                        #     G /= G.sum()
                        #     G = torch.tensor(G[:,:,None]).to(device).type(torch_type)
                        #     tmp_corr2 = torch.fft.ifftshift(torch.fft.ifft2(torch.fft.fft2(G,dim=(0,1))*torch.fft.fft2(depl,dim=(0,1)),dim=(0,1)),dim=(0,1)).real
                        #     tmp_corr2 = torch.abs(tmp_corr2.sum(2))/((depl**2).sum(2)+1e-6)
                        #     G = np.zeros((n1,n2))
                        #     G[n1//2-1,n2//2-1] = 1
                        #     G[n1//2,n2//2] = 1
                        #     G /= G.sum()
                        #     G = torch.tensor(G[:,:,None]).to(device).type(torch_type)
                        #     tmp_corr2 = torch.fft.ifftshift(torch.fft.ifft2(torch.fft.fft2(G,dim=(0,1))*torch.fft.fft2(depl,dim=(0,1)),dim=(0,1)),dim=(0,1)).real
                        #     tmp_corr2 = torch.abs(tmp_corr2.sum(2))/((depl**2).sum(2)+1e-6)

                        #     loss += lamb_local_smooth*(tmp_corr1.mean()+tmp_corr2.mean())
                        #     loss_regul_local_smooth.append((lamb_local_smooth*(tmp_corr1.mean()+tmp_corr2.mean())).item())
                                        
                # global deformations
                if learn_global:
                    if lamb_rot!=0 or lamb_shifts!=0:
                        for ii in idx_loader.detach().cpu().numpy():
                            loss += lamb_shifts*torch.abs(shift_est[ii]()).sum()
                            loss += lamb_rot*torch.abs(rot_est[ii]()).sum()
                            loss_regul_shifts.append((lamb_shifts*torch.abs(shift_est[ii]()).sum()).item())
                            loss_regul_rot.append((lamb_rot*torch.abs(rot_est[ii]()).sum()).item())
            
            if learn_volume and lamb_volume!=0:
                V_est = impl_volume(grid_class_us.grid_ball3d_t.clone())
                barycenter_est = (V_est.reshape(-1,1)*grid_class_us.grid_ball3d_t.clone()).mean(0)
                loss += lamb_volume*torch.abs(barycenter_est-barycenter_true).sum()
                loss_regul_volume.append(lamb_volume*torch.abs(barycenter_est-barycenter_true).sum().item())

            loss.backward()
            optimizer_volume.step()
            if learn_deformations:
                optimizer_deformations.step()

            loss_tot.append(loss.item())

        scheduler_volume.step()
        scheduler_deformation.step()

        if ep%Ntest==0 :#and ep!=0:
            t_test_ = time.time()
            with torch.no_grad():
                gr = grid_class.grid3d_t/2+0.5
                gr[:,2] *= n3/max(n1,n2) 
                gr[:,2] += 0.5-(n3/max(n1,n2))/2
                V_est = impl_volume(gr).type(torch_type).reshape(n1,n2,n3)/n3
                out = mrcfile.new(path_save+"/training/V_est_iter.mrc",np.moveaxis(V_est.detach().cpu().numpy(),2,0),overwrite=True)
                out.close() 
                out = mrcfile.new(path_save+"/training/V_true_iter.mrc",np.moveaxis(V_t.detach().cpu().numpy(),2,0),overwrite=True)
                out.close() 

                V_ours = V_est.detach().cpu().numpy()
                V_ours /= V_ours.sum()

                fsc_ours = utils_ricardo.FSC(V_,V_ours)
                
                if len(np.where(fsc_ours[:,0]>0.5)[0]) == 0:
                    fsc_5 = 0.
                else:    
                    fsc_5 = 1/x_fsc[np.where(fsc_ours[:,0]>0.5)][-1]
                if len(np.where(fsc_ours[:,0]>0.143)[0]) == 0:
                    fsc_143 = 0.
                else:
                    fsc_143 = 1/x_fsc[np.where(fsc_ours[:,0]>0.143)][-1]

                SNR_est_vol = reconstruction.SNR(V_.reshape(-1) , V_ours.reshape(-1))

                plt.figure(1)
                plt.clf()
                plt.plot(x_fsc,fsc_ours,label='Est.')
                plt.plot(x_fsc,fsc_FBP,label='FBP')
                plt.plot(x_fsc,fsc_FBP_no_deformed,label='FBP no deformed')
                plt.xlabel("1/Resolution")
                plt.ylabel("FSC")
                plt.legend()
                plt.savefig(os.path.join(path_save,'training','FSC_ep_{}.png'.format(ep)))

                plt.figure(1)
                plt.clf()
                plt.plot(np.array(loss_tot))
                plt.legend()
                plt.savefig(os.path.join(path_save,'training','loss_ep_{}.png'.format(ep)))



                loss_current_epoch = np.mean(loss_tot[-Ntest:])
                print('Epoch: {}, loss: {:2.3}, loss_avg: {:2.3}, SNR vol: {:2.3}, res. (Cref): {:2.3}, res. (143): {:2.3}'.format(
                    ep,loss.item(),loss_current_epoch,SNR_est_vol, fsc_5, fsc_143))
                l_fid = np.mean(loss_data_fidelity[-Ntest:])
                l_v = np.mean(loss_regul_volume[-Ntest:])
                l_sh = np.mean(loss_regul_shifts[-Ntest:])
                l_rot = np.mean(loss_regul_rot[-Ntest:])
                l_loc = np.mean(loss_regul_local_ampl[-Ntest:])
                l_loc_smooth = np.mean(loss_regul_local_smooth[-Ntest:])
                print("                  Loss data fidelity: {:2.3}, regul volume: {:2.3}, regul shifts: {:2.3}, regul inplane: {:2.3}, regul local: {:2.3}, time: {:2.3}".format(l_fid,
                        l_v,l_sh,l_rot,l_loc,time.time()-t0))
                

                err_shift, err_shift_init, err_rot, err_rot_init, err_local, err_local_init = reconstruction.computeDeformationScore(grid_class_us.grid2d_t,angles_t,
                                                                                                                                     shift_est,rot_est,
                                                                                                                                     implicit_deformation_list,
                                                                                                                                     affine_tr,local_tr,
                                                                                                                                     n1,scale=deformationScale)

                
            with torch.no_grad():
                Nangles_tmp = 10
                for kk in range(Nangles_tmp):
                    tmp_ind = np.round(np.linspace(0,Nangles-1,Nangles_tmp)).astype(np.uint8)
                    angles_tmp = np.linspace(-angle_bound,angle_bound,Nangles_tmp)
                    angles_tmp_t = torch.tensor(angles_tmp).type(torch_type).to(device)
                    # proj_est = sample_implicit(impl_volume,grid_class_us.grid3d_t,angles_tmp_t[kk],rot_deform=None,
                    #                            shift_deform=None,local_deform=None,scale=deformationScale).reshape(n1//us,n2//us,n3//us)*mollifier.mollify3d()
                    

                    rays, _, _ = generate_ray(grid_class.grid2d_t,angles_tmp_t[kk:kk+1],ray_length,randomZ = 2,zmax=z_max)
                    rays = rays.unsqueeze(0)
                    outputValues,support = sample_implicit_batch_lowComp(impl_volume,rays,angles_tmp_t[kk:kk+1],
                        rot_deformSet=None,shift_deformSet=None,local_deformSet=None,
                        scale=deformationScale,range=inputRange,zlimit=n3/max(n1,n2))

                    support = support.reshape(outputValues.shape[0],outputValues.shape[1],-1)
                    outputValues = outputValues.type(torch_type)
                    proj_est = torch.sum(support*outputValues,2)/n3
                    proj_est = proj_est.reshape(n1,n2)
                    plt.figure(1)
                    plt.clf()
                    plt.subplot(3,1,1)
                    plt.imshow((projections_clean[tmp_ind[kk]]).detach().cpu().numpy(),cmap='gray')
                    plt.title("{}".format(angles_tmp[kk]))
                    plt.subplot(3,1,2)
                    plt.imshow((projections_noisy[tmp_ind[kk]]).detach().cpu().numpy(),cmap='gray')
                    plt.subplot(3,1,3)
                    plt.imshow((proj_est).detach().cpu().numpy(),cmap='gray')
                    plt.savefig(os.path.join(path_save,'projections','training','recons_ep_{}_{}.png'.format(ep,kk)))

                    tmp = (proj_est*mollifier.mollify2d()).detach().cpu().numpy()
                    tmp[np.isfinite(tmp)==False] = 0
                    tmp = (tmp - tmp.max())/(tmp.max()-tmp.min())
                    tmp = np.floor(255*tmp).astype(np.uint8)
                    imageio.imwrite(os.path.join(path_save,'projections','training','est_ep_{}_{}.png'.format(ep,kk)),tmp)

                    ind_ = np.arange(0,n3,Nangles_tmp)
                    tmp = (V_est[:,:,ind_[kk]]).detach().cpu().numpy()
                    tmp[np.isfinite(tmp)==False] = 0
                    tmp = (tmp - tmp.max())/(tmp.max()-tmp.min())
                    tmp = np.floor(255*tmp).astype(np.uint8)
                    imageio.imwrite(os.path.join(path_save,'projections','volume','est_ep_{}_{}.png'.format(ep,ind_[kk])),tmp)

                if(use_deformation_estimation):
                    print("   With est. deformations || Err shift: {:2.3}+/-{:2.3} -- Err rot: {:2.3}+/-{:2.3} -- Err local: {:2.3}+/-{:2.3}".format(err_shift.mean(),err_shift.std(),err_rot.mean(),err_rot.std(),err_local.mean(),err_local.std()))
                    print("Without est. deformations || Err shift: {:2.3}+/-{:2.3} -- Err rot: {:2.3}+/-{:2.3} -- Err local: {:2.3}+/-{:2.3}".format(err_shift_init.mean(),err_shift_init.std(),err_rot_init.mean(),err_rot_init.std(),err_local_init.mean(),err_local_init.std()))

                    kk = Nangles_//2
                    utils_display.display_local(implicit_deformation_list[kk],field_true=local_tr[kk],Npts=(20,20),img_path=path_save+"deformations/training/quiver_middle_"+str(ep),
                                                img_type='.pdf',scale=1,alpha=0.8,width=0.002)
                    if not os.path.exists(path_save+"deformations/training/"+str(ep)+"/"):
                        os.makedirs(path_save+"deformations/training/"+str(ep)+"/")
                    utils_display.display_local_movie(implicit_deformation_list,field_true=local_tr,Npts=(20,20),
                                                      img_path=path_save+"deformations/training/"+str(ep)+"/",img_type='.png',
                                                      scale=1/10,alpha=0.8,width=0.002)
                                
            with torch.no_grad():
                for kk in range(len(choosenLocations_all)):

                    plt.figure(1)
                    plt.clf()
                    plt.imshow(current_sampling[kk])
                    plt.colorbar()

                    # plt.clf()
                    # for ii in range(len(choosenLocations_all[angles[kk]])):
                    #     pts = choosenLocations_all[angles[kk]][ii]
                    #     plt.scatter(pts[:,0],pts[:,1],s=1,c='b',marker='+')

                    if not os.path.exists(path_save+"projections/sampling/"+str(ep)+"/"):
                        os.makedirs(path_save+"projections/sampling/"+str(ep)+"/")
                    plt.savefig(os.path.join(path_save,'projections','sampling',str(ep),'angle_{}.png'.format(kk)))
                    
                    
            if(use_deformation_estimation):
                torch.save({
                'shift_est': shift_est,
                'rot_est': rot_est,
                'local_deformation_network': implicit_deformation_list,
                'implicit_volume': impl_volume.state_dict(),
                }, os.path.join(path_save, 'model_everything_joint_batch.pt'))
            else:
                torch.save({
                'shift_est': shift_est,
                'rot_est': rot_est,
                'implicit_volume': impl_volume.state_dict(),
                }, os.path.join(path_save, 'model_everything_joint_batch.pt'))
            t_test.append(time.time()-t_test_)
else:
    checkpoint = torch.load(os.path.join(path_save, 'model_everything_joint_batch.pt'),map_location=device)
    impl_volume.load_state_dict(checkpoint['implicit_volume'])
    shift_est = checkpoint['shift_est']
    rot_est = checkpoint['rot_est']
    implicit_deformation_list = checkpoint['local_deformation_network']


with torch.no_grad():
    if(use_deformation_estimation):
        torch.save({
        'shift_est': shift_est,
        'rot_est': rot_est,
        'local_deformation_network': implicit_deformation_list,
        'implicit_volume': impl_volume.state_dict(),
        }, os.path.join(path_save, 'model_everything_joint_batch.pt'))
    else:
        torch.save({
        'shift_est': shift_est,
        'rot_est': rot_est,
        'implicit_volume': impl_volume.state_dict(),
        }, os.path.join(path_save, 'model_everything_joint_batch.pt'))




